{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128b38ec",
   "metadata": {},
   "source": [
    "# Data Analysis (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966cb662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947c9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('Spark App').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfb8eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection_cext.CMySQLConnection object at 0x000001D82CBD1D30>\n"
     ]
    }
   ],
   "source": [
    "# Connect to MySQL\n",
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"mysql\"\n",
    ")\n",
    "print(mydb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24abd12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cdacnoida',)\n",
      "('classwork',)\n",
      "('exercise',)\n",
      "('healthcare_system',)\n",
      "('information_schema',)\n",
      "('mysql',)\n",
      "('performance_schema',)\n",
      "('practice',)\n",
      "('project',)\n",
      "('sakila',)\n",
      "('sys',)\n",
      "('vitawa_sevakendra',)\n",
      "('world',)\n"
     ]
    }
   ],
   "source": [
    "# List Databases\n",
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"show databases\")\n",
    "myresult = mycursor.fetchall()\n",
    "for x in myresult:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "701d72ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successfully established to 8.0.28\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to database\n",
    "import mysql\n",
    "import mysql.connector\n",
    "connection=mysql.connector.Connect(host='localhost',database='healthcare_system',user='root',password='mysql')\n",
    "if connection.is_connected():\n",
    "    db_info = connection.get_server_info()\n",
    "    print(\"Connection successfully established to\",db_info)\n",
    "else:\n",
    "    print(\"Connection Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d322f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to ('healthcare_system',)\n"
     ]
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(\"select database();\")\n",
    "record=cursor.fetchone()\n",
    "print(\"You are connected to\",record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc20f188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('claims',)\n",
      "('disease',)\n",
      "('grp_subgrp',)\n",
      "('grps',)\n",
      "('hospital',)\n",
      "('patient',)\n",
      "('subgroup',)\n",
      "('subscriber',)\n"
     ]
    }
   ],
   "source": [
    "# List Tables\n",
    "cursor=connection.cursor()\n",
    "cursor.execute(\"Show Tables;\")\n",
    "record=cursor.fetchall()\n",
    "for x in record:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b2d42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sql functions\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "400891e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|   subgrp_id|   string|   null|\n",
      "|  disease_id|      int|   null|\n",
      "|disease_name|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the disease table\n",
    "df=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/disease.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df.createOrReplaceTempView(\"disease\")\n",
    "spark.sql(\"desc disease\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "492de992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|       country|   string|   null|\n",
      "|premium_return|      int|   null|\n",
      "|       pincode|      int|   null|\n",
      "|        grp_id|   string|   null|\n",
      "|      grp_name|   string|   null|\n",
      "|      grp_type|   string|   null|\n",
      "|          city|   string|   null|\n",
      "|      year_est|      int|   null|\n",
      "|        grp_sk|   string|   null|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the groups table\n",
    "df1=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/groups.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df1.createOrReplaceTempView(\"groups\")\n",
    "spark.sql(\"desc groups\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "524d4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    s_id|   string|   null|\n",
      "|    g_id|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the grp_subgrp table\n",
    "df2=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/group_subgroup.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df2.createOrReplaceTempView(\"grp_subgrp\")\n",
    "spark.sql(\"desc grp_subgrp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4b986a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|     s_key|      int|   null|\n",
      "|    sub_id|   string|   null|\n",
      "|first_name|   string|   null|\n",
      "| last_name|   string|   null|\n",
      "|    street|   string|   null|\n",
      "|birth_date|timestamp|   null|\n",
      "|    gender|   string|   null|\n",
      "|     phone|   string|   null|\n",
      "|   country|   string|   null|\n",
      "|      city|   string|   null|\n",
      "|   pincode|      int|   null|\n",
      "| subgrp_id|   string|   null|\n",
      "|  elig_ind|   string|   null|\n",
      "|    e_date|timestamp|   null|\n",
      "|    t_date|timestamp|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subscriber table\n",
    "df3=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/subscriber.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df3.createOrReplaceTempView(\"subscriber\")\n",
    "spark.sql(\"desc subscriber\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c1fb68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|  hospital_id|   string|   null|\n",
      "|hospital_name|   string|   null|\n",
      "|         city|   string|   null|\n",
      "|        state|   string|   null|\n",
      "|      country|   string|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the hospital table\n",
    "df4=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/hospital.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df4.createOrReplaceTempView(\"hospital\")\n",
    "spark.sql(\"desc hospital\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d7b48ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|  patient_id|      int|   null|\n",
      "|patient_name|   string|   null|\n",
      "|      gender|   string|   null|\n",
      "|  birth_date|timestamp|   null|\n",
      "|       phone|   string|   null|\n",
      "|disease_name|   string|   null|\n",
      "|        city|   string|   null|\n",
      "| hospital_id|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the patient table\n",
    "df5=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/patient.csv\",sep=\",\",inferSchema=True,header=True)\n",
    "df5.createOrReplaceTempView(\"patient\")\n",
    "spark.sql(\"desc patient\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "480e7de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|      subgrp_id|   string|   null|\n",
      "|    subgrp_name|   string|   null|\n",
      "|monthly_premium|      int|   null|\n",
      "|      subgrp_sk|   string|   null|\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the subgroup table\n",
    "df6=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/subgroup.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df6.createOrReplaceTempView(\"subgroup\")\n",
    "spark.sql(\"desc subgroup\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52f61cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------+\n",
      "|      col_name|data_type|comment|\n",
      "+--------------+---------+-------+\n",
      "|      claim_id|      int|   null|\n",
      "|    patient_id|      int|   null|\n",
      "|  disease_name|   string|   null|\n",
      "|        sub_id|   string|   null|\n",
      "|claimed_or_rej|   string|   null|\n",
      "|    claim_type|   string|   null|\n",
      "|  claim_amount|      int|   null|\n",
      "|    claim_date|timestamp|   null|\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the claims table\n",
    "df7=spark.read.csv(\"C:/Users/shard/OneDrive/CDAC/Project/OurWork/claims.csv\",sep=\";\",inferSchema=True,header=True)\n",
    "df7.createOrReplaceTempView(\"claims\")\n",
    "spark.sql(\"desc claims\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08cc3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|   disease_name|max|\n",
      "+---------------+---+\n",
      "|       Glaucoma|  3|\n",
      "|   Head banging|  3|\n",
      "|        Anthrax|  3|\n",
      "|   Galactosemia|  3|\n",
      "|    Pet allergy|  3|\n",
      "|Phenylketonuria|  3|\n",
      "|         Stroke|  2|\n",
      "|       Diabetes|  2|\n",
      "| Bladder cancer|  2|\n",
      "|Fanconi anaemia|  2|\n",
      "+---------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which disease having maximum number of claims.\n",
    "results = spark.sql(\"select disease_name, count(claim_id) as max from claims group by disease_name order by max desc\")\n",
    "results.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query1.csv')\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52a2007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---+\n",
      "|         birth_date|CurrentDate|age|\n",
      "+-------------------+-----------+---+\n",
      "|1924-06-30 00:00:00| 2022-09-23| 98|\n",
      "|1948-12-20 00:00:00| 2022-09-23| 74|\n",
      "|1980-04-16 00:00:00| 2022-09-23| 42|\n",
      "|1969-09-25 00:00:00| 2022-09-23| 53|\n",
      "|1946-05-01 00:00:00| 2022-09-23| 76|\n",
      "|1967-10-02 00:00:00| 2022-09-23| 55|\n",
      "|1925-03-05 00:00:00| 2022-09-23| 97|\n",
      "|1945-05-06 00:00:00| 2022-09-23| 77|\n",
      "|1925-06-12 00:00:00| 2022-09-23| 97|\n",
      "|1955-01-22 00:00:00| 2022-09-23| 67|\n",
      "|1964-04-29 00:00:00| 2022-09-23| 58|\n",
      "|1991-11-11 00:00:00| 2022-09-23| 31|\n",
      "|1981-01-25 00:00:00| 2022-09-23| 41|\n",
      "|1966-07-24 00:00:00| 2022-09-23| 56|\n",
      "|1933-11-20 00:00:00| 2022-09-23| 89|\n",
      "|1996-10-15 00:00:00| 2022-09-23| 26|\n",
      "|1935-09-16 00:00:00| 2022-09-23| 87|\n",
      "|1924-11-09 00:00:00| 2022-09-23| 98|\n",
      "|1923-09-15 00:00:00| 2022-09-23| 99|\n",
      "|1920-11-13 00:00:00| 2022-09-23|102|\n",
      "+-------------------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find those Subscribers having age less than 30 and they subscribe any subgroup\n",
    "subscriber_ages = spark.sql(\"select birth_date,current_date() as CurrentDate,\\\n",
    "          year(current_date())-year(birth_date) as age from subscriber\")\n",
    "subscriber_ages.sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query2.csv')\n",
    "subscriber_ages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb64ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscribers having age less than 30 -->  4\n"
     ]
    }
   ],
   "source": [
    "res = subscriber_ages.filter(subscriber_ages.age < 30).count()\n",
    "print(\"Subscribers having age less than 30 --> \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e051ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  g_id|max|\n",
      "+------+---+\n",
      "|GRP143|  2|\n",
      "|GRP147|  2|\n",
      "|GRP104|  2|\n",
      "|GRP105|  1|\n",
      "|GRP108|  1|\n",
      "|GRP131|  1|\n",
      "|GRP102|  1|\n",
      "|GRP103|  1|\n",
      "|GRP112|  1|\n",
      "|GRP101|  1|\n",
      "|GRP123|  1|\n",
      "|GRP114|  1|\n",
      "|GRP127|  1|\n",
      "|GRP133|  1|\n",
      "|GRP122|  1|\n",
      "|GRP138|  1|\n",
      "|GRP148|  1|\n",
      "|GRP142|  1|\n",
      "|GRP157|  1|\n",
      "|GRP126|  1|\n",
      "+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which group has maximum subgroups.\n",
    "sparkdf = spark.sql(\"select g_id,count(s_id) as max from grp_subgrp group by g_id order by max desc\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query3.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8892c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|       hospital_name|total_patient|\n",
      "+--------------------+-------------+\n",
      "|   Manipal Hospitals|            9|\n",
      "|Apollo Hospitals ...|            8|\n",
      "|Medanta The Medicity|            7|\n",
      "|Jaslok Hospital a...|            6|\n",
      "|Indraprastha Apol...|            5|\n",
      "|Fortis Hospital M...|            4|\n",
      "|PGIMER - Postgrad...|            4|\n",
      "|Apollo Hospital -...|            4|\n",
      "|Bombay Hospital &...|            3|\n",
      "|Yashoda Hospital ...|            3|\n",
      "|Apollo Health Cit...|            3|\n",
      "|King Edward Memor...|            3|\n",
      "|Lilavati Hospital...|            2|\n",
      "|The Christian Med...|            2|\n",
      "|Fortis Hiranandan...|            2|\n",
      "|Fortis Flt. Lt. R...|            1|\n",
      "|P. D. Hinduja Nat...|            1|\n",
      "|Sir Ganga Ram Hos...|            1|\n",
      "|All India Institu...|            1|\n",
      "|Breach Candy Hosp...|            1|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out hospital which serve most number of patients\n",
    "sparkdf = spark.sql(\"select hospital_name,count(patient_id) as total_patient \\\n",
    "          from hospital join patient on hospital.hospital_id=patient.hospital_id \\\n",
    "          group by hospital_name order by total_patient desc\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query4.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4594ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|        subgrp_name|cunt|\n",
      "+-------------------+----+\n",
      "|            Therapy|  14|\n",
      "|              Viral|  11|\n",
      "|Deficiency Diseases|  11|\n",
      "|         Hereditary|  11|\n",
      "|          Allergies|  10|\n",
      "|         Physiology|  10|\n",
      "|           Accident|  10|\n",
      "|             Cancer|   9|\n",
      "|     Self inflicted|   7|\n",
      "| Infectious disease|   7|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out which subgroups subscribe most number of times\n",
    "sparkdf = spark.sql(\"select subgrp_name, \\\n",
    "          count(sub_id) as cunt from subgroup join \\\n",
    "          subscriber on subgroup.subgrp_id = subscriber.subgrp_id  group by subgrp_name order by cunt desc\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query5.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ec865d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|claimed_or_rej|count(claim_id)|\n",
      "+--------------+---------------+\n",
      "|             Y|             18|\n",
      "|             N|             52|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out total number of claims which were rejected\n",
    "sparkdf = spark.sql(\"select claimed_or_rej,count(claim_id) from claims group by claimed_or_rej\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query6.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eadf5584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|        city|maxclaim|\n",
      "+------------+--------+\n",
      "|      Mysore|       2|\n",
      "|    Amravati|       2|\n",
      "|   Kamarhati|       2|\n",
      "|    Jabalpur|       2|\n",
      "|Bihar Sharif|       2|\n",
      "|   Ghaziabad|       2|\n",
      "|       Morbi|       2|\n",
      "|  Karimnagar|       2|\n",
      "|   Bangalore|       1|\n",
      "|     Udaipur|       1|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From where most claims are comming (city)\n",
    "sparkdf = spark.sql(\"select patient.city,count(claim_id) as maxclaim from patient join claims on \\\n",
    "          claims.patient_id = patient.patient_id group by patient.city order by maxclaim desc limit 10\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query7.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8929698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|grp_type|count(grp_id)|\n",
      "+--------+-------------+\n",
      "|   Govt.|            7|\n",
      "| Private|           51|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which groups of policies subscriber subscribe mostly Goverment or private\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_id) from groups group by grp_type\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query8.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d46591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark\\python\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|grp_type|count(g_id)|\n",
      "+--------+-----------+\n",
      "|   Govt.|         35|\n",
      "| Private|        347|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = spark.sql(\"select grp_subgrp.g_id from subscriber \\\n",
    "          join grp_subgrp on grp_subgrp.s_id = subscriber.subgrp_id\")\n",
    "\n",
    "res.registerTempTable(\"grp_tble\")\n",
    "\n",
    "sparkdf = spark.sql(\"select grp_type,count(grp_tble.g_id) from groups \\\n",
    "          join grp_tble on groups.grp_id = grp_tble.g_id group by grp_type\")\n",
    "\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query9.csv')\n",
    "\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2519ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|subgrp_id|avg(monthly_premium)|\n",
      "+---------+--------------------+\n",
      "|     S101|              3000.0|\n",
      "|     S102|              1000.0|\n",
      "|     S103|              2000.0|\n",
      "|     S104|              1500.0|\n",
      "|     S105|              2300.0|\n",
      "|     S106|              1200.0|\n",
      "|     S107|              3200.0|\n",
      "|     S108|              1500.0|\n",
      "|     S109|              2000.0|\n",
      "|     S110|              1000.0|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average monthly premium subscriber pay to insurance company subgroup.\n",
    "sparkdf = spark.sql(\"select subgroup.subgrp_id,avg(monthly_premium) from subscriber right join \\\n",
    "          subgroup on subscriber.subgrp_id = subgroup.subgrp_id group by \\\n",
    "          subgroup.subgrp_id order by subgroup.subgrp_id\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query10.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ce813d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|grp_id|premium_return|\n",
      "+------+--------------+\n",
      "|GRP147|         99000|\n",
      "|GRP131|         99000|\n",
      "|GRP123|         99000|\n",
      "|GRP118|         97000|\n",
      "|GRP154|         95000|\n",
      "|GRP133|         93000|\n",
      "|GRP157|         92000|\n",
      "|GRP134|         90000|\n",
      "|GRP143|         90000|\n",
      "|GRP106|         89000|\n",
      "|GRP129|         88000|\n",
      "|GRP152|         87000|\n",
      "|GRP153|         86000|\n",
      "|GRP150|         84000|\n",
      "|GRP124|         81000|\n",
      "|GRP122|         79000|\n",
      "|GRP115|         79000|\n",
      "|GRP121|         78000|\n",
      "|GRP109|         78000|\n",
      "|GRP101|         72000|\n",
      "+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out Which group is most profitable\n",
    "sparkdf = spark.sql(\"select grp_id,premium_return from groups order by premium_return desc\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query11.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca7f9b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+\n",
      "|patient_id|patient_name|age|\n",
      "+----------+------------+---+\n",
      "|    194166|          NA|  9|\n",
      "|    197441|    Deependu| 18|\n",
      "+----------+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all the patients below age of 18 who admit for cancer\n",
    "sparkdf = spark.sql(\"select patient_id, patient_name,\\\n",
    "          year(current_date())-year(birth_date) as age from patient \\\n",
    "          where disease_name like '%cancer' order by age limit 2\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query12.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77a10809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+\n",
      "|patient_name|gender|         birth_date|\n",
      "+------------+------+-------------------+\n",
      "|      Harbir|Female|1960-02-24 00:00:00|\n",
      "|     Ujjawal|  Male|1965-12-31 00:00:00|\n",
      "|     Devnath|Female|1982-02-22 00:00:00|\n",
      "|       Aakar|Female|1990-04-24 00:00:00|\n",
      "|          NA|  Male|1955-06-03 00:00:00|\n",
      "|  Dharmadaas|  Male|1964-10-25 00:00:00|\n",
      "|          NA|Female|1953-04-04 00:00:00|\n",
      "|     Bhagvan|Female|2011-02-26 00:00:00|\n",
      "|          NA|Female|1959-01-06 00:00:00|\n",
      "|       Umang|Female|2017-02-26 00:00:00|\n",
      "|  Vaijayanti|  Male|1969-04-06 00:00:00|\n",
      "|       Ekant|  Male|1969-11-01 00:00:00|\n",
      "|      Gensho|  Male|1991-07-27 00:00:00|\n",
      "|   Anjushree|  Male|1982-06-28 00:00:00|\n",
      "|       Saroj|Female|1953-07-21 00:00:00|\n",
      "|          NA|  Male|1956-04-04 00:00:00|\n",
      "|  Chitranjan|Female|2020-10-27 00:00:00|\n",
      "|          NA|  Male|2013-10-30 00:00:00|\n",
      "|       Lalit|Female|1978-04-30 00:00:00|\n",
      "|      Kishan|  Male|1955-06-30 00:00:00|\n",
      "+------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List patients who have cashless insurance and have total charges greater than or equal for Rs. 50,000.\n",
    "sparkdf = spark.sql(\"select patient_name, gender, birth_date \\\n",
    "          from patient join claims on patient.patient_id = claims.patient_id \\\n",
    "          where claim_amount >= 50000 and claim_type = 'claims of value'\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query13.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a87667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|patient_name|\n",
      "+------------+\n",
      "|     Upasana|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List female patients over the age of 40 that have undergone knee surgery in the past year\n",
    "sparkdf = spark.sql(\"select patient_name from patient \\\n",
    "                    where gender = 'Female' and disease_name = 'Heart Attack'\")\n",
    "sparkdf.toPandas().to_csv('C:/Users/shard/OneDrive/CDAC/Project/OurWork/Spark/query14.csv')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b6bfb",
   "metadata": {},
   "source": [
    "# Thankyou!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
